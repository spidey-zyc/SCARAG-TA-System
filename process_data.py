import os
import argparse
from document_loader import DocumentLoader
from text_splitter import TextSplitter
from vector_store import VectorStore
from config import DATA_DIR, CHUNK_SIZE, CHUNK_OVERLAP, VECTOR_DB_PATH

import base64
from tqdm import tqdm
from rag_agent import RAGAgent # ç”¨äºè°ƒç”¨ Vision API
import argparse


# ä½ çš„åŸºç¡€æ•°æ®è·¯å¾„
BASE_DATA_DIR = os.path.join(".", "data")

def encode_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

def process_images_with_vision_model(chunks,theme_name):
    """
    éå†æ–‡æ¡£å—ï¼Œæ‰¾åˆ°å›¾ç‰‡å—ï¼Œè°ƒç”¨è§†è§‰æ¨¡å‹ç”Ÿæˆæè¿°
    """
    agent = RAGAgent(initial_theme=theme_name) # å®ä¾‹åŒ–ä»¥ä½¿ç”¨å…¶ä¸­çš„ vision_client
    processed_chunks = []
    
    print("\nğŸ‘ï¸ æ­£åœ¨è¿›è¡Œå›¾ç‰‡è¯­ä¹‰åˆ†æä¸æè¿°ç”Ÿæˆ (è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´)...")
    
    image_chunks = [c for c in chunks if c.get("is_image")]
    text_chunks = [c for c in chunks if not c.get("is_image")]
    
    # å…ˆæŠŠçº¯æ–‡æœ¬æ”¾è¿›å»
    processed_chunks.extend(text_chunks)
    
    for chunk in tqdm(image_chunks, desc="åˆ†æå›¾ç‰‡", unit="å¼ "):
        try:
            img_path = chunk["image_path"]
            if not os.path.exists(img_path):
                continue
                
            base64_img = encode_image(img_path)
            
            # ä½¿ç”¨ Agent ä¸­å·²æœ‰çš„æ–¹æ³•ç”Ÿæˆæè¿°
            # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬å¤ç”¨ understand_imageï¼Œä½†æç¤ºè¯æ˜¯é’ˆå¯¹é€šç”¨æœç´¢ä¼˜åŒ–çš„
            description = agent.understand_image(base64_img)
            
            if description:
                # æ›´æ–°å†…å®¹ï¼šåŠ ä¸Šæ–‡ä»¶åä½œä¸ºå‰ç¼€ï¼Œå¢å¼ºæ£€ç´¢ç›¸å…³æ€§
                final_content = f"ã€å›¾ç‰‡å†…å®¹æè¿°ã€‘(æ–‡ä»¶: {chunk['filename']}, é¡µç : {chunk['page_number']})\n{description}"
                chunk["content"] = final_content
                # ç§»é™¤ is_image æ ‡è®°ï¼Œæˆ–è€…ä¿ç•™å®ƒç”¨äºåç»­é€»è¾‘ï¼Œè¿™é‡Œæˆ‘ä»¬è¦ä¿ç•™ image_path
                processed_chunks.append(chunk)
                
        except Exception as e:
            print(f"å¤„ç†å›¾ç‰‡ {chunk.get('image_path')} å¤±è´¥: {e}")
    
    return processed_chunks









def main():
    # 1. è§£æå‚æ•°
    parser = argparse.ArgumentParser()
    parser.add_argument("--theme", type=str, default="Default", help="æŒ‡å®šä¸»é¢˜æ–‡ä»¶å¤¹") # é»˜è®¤ä¸º Default
    parser.add_argument("--incremental", action="store_true", help="å¢é‡æ›´æ–°æ¨¡å¼")
    parser.add_argument("--text_only", action="store_true", help="ä»…å¤„ç†æ–‡æœ¬(å¿«é€Ÿæ¨¡å¼)")
    parser.add_argument("--image_only", action="store_true", help="ä»…å¤„ç†å›¾ç‰‡(åå°æ¨¡å¼)")
    args = parser.parse_args()

    # 2. ç¡®å®šè·¯å¾„
    # å¦‚æœæ˜¯ Defaultï¼Œå¯èƒ½æŒ‡å‘æ ¹ data ç›®å½•ï¼Œæˆ–è€… data/Defaultï¼Œæ ¹æ®ä½ çš„æ–‡ä»¶ç»“æ„å†³å®š
    # è¿™é‡Œå‡è®¾ data ä¸‹é¢å…¨æ˜¯å­æ–‡ä»¶å¤¹
    theme_name = args.theme
    if theme_name == "default":
        # å¦‚æœä½ æƒ³æŠŠ data æ ¹ç›®å½•ä½œä¸ºé»˜è®¤
        target_dir = BASE_DATA_DIR 
    else:
        target_dir = os.path.join(BASE_DATA_DIR, theme_name)

    if not os.path.exists(target_dir):
        print(f"ç›®å½•ä¸å­˜åœ¨: {target_dir}")
        return

    print(f"ğŸ“‚ å¤„ç†ç›®å½•: {target_dir}")
    print(f"ğŸ“š ç›®æ ‡ä¸»é¢˜(Collection): {theme_name}")

    # 3. åˆå§‹åŒ– (ä¼ å…¥ collection_name)
    loader = DocumentLoader(data_dir=target_dir)
    splitter = TextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
    
    # ã€å…³é”®ä¿®æ”¹ã€‘åœ¨è¿™é‡Œä¼ å…¥ theme_name
    vector_store = VectorStore(
        db_path=VECTOR_DB_PATH, 
        collection_name=theme_name 
    )
    
    # 4. æ¸…ç†ç­–ç•¥ (é’ˆå¯¹å½“å‰ collection)
    if args.image_only:
        print("â• åå°å›¾ç‰‡å¤„ç†æ¨¡å¼ï¼šå¼ºåˆ¶ä½¿ç”¨å¢é‡æ›´æ–°...")
        args.incremental = True

    if not args.incremental:
        print(f"ğŸ§¹ å…¨é‡æ¨¡å¼ï¼šæ¸…ç©ºä¸»é¢˜ã€{theme_name}ã€‘çš„æ•°æ®...")
        vector_store.clear_collection() # è¿™åªä¼šæ¸…ç©ºå½“å‰ä¸»é¢˜ï¼Œä¸ä¼šå½±å“å…¶ä»–ä¸»é¢˜
    else:
        print("â• å¢é‡æ¨¡å¼ï¼šä¿ç•™æ—§æ•°æ®...")

    # 5. åŠ è½½æ–‡æ¡£
    documents = loader.load_all_documents(specific_dir=target_dir)
    if not documents:
        print("âš ï¸ è¯¥ç›®å½•ä¸‹æ²¡æœ‰æ–‡æ¡£")
        return

    # 6. åˆ†æµå¤„ç†
    all_chunks = []
    
    # --- åˆ†æ”¯ A: å¤„ç†æ–‡æœ¬ (åªè¦æ²¡å¼€å¯ image_only å°±è·‘æ–‡æœ¬) ---
    if not args.image_only:
        print("ğŸš€ [Text Mode] æ­£åœ¨å¤„ç†æ–‡æœ¬...")
        raw_text_docs = [d for d in documents if not d.get("is_image")]
        text_chunks = splitter.split_documents(raw_text_docs)
        all_chunks.extend(text_chunks)
    else:
        print("â© [Text Mode] è·³è¿‡æ–‡æœ¬å¤„ç†")

    # --- åˆ†æ”¯ B: å¤„ç†å›¾ç‰‡ (åªè¦æ²¡å¼€å¯ text_only å°±è·‘å›¾ç‰‡) ---
    if not args.text_only:
        print("ğŸ‘ï¸ [Vision Mode] æ­£åœ¨åˆ†æå›¾ç‰‡...")
        raw_image_docs = [d for d in documents if d.get("is_image")]
        
        image_chunks_formatted = []
        for i, img_doc in enumerate(raw_image_docs):
            img_doc["chunk_id"] = f"img_{i}"
            image_chunks_formatted.append(img_doc)
        
        if image_chunks_formatted:
            processed_imgs = process_images_with_vision_model(image_chunks_formatted,theme_name=theme_name)
            all_chunks.extend(processed_imgs)
    else:
        print("â© [Vision Mode] è·³è¿‡å›¾ç‰‡å¤„ç† (å°†åœ¨åå°è¿è¡Œ)")

    # 7. å†™å…¥æ•°æ®åº“
    if all_chunks:
        print(f"ğŸ’¾ å†™å…¥ {len(all_chunks)} æ¡æ•°æ®...")
        
        # æ¸…æ´— metadata é˜²æ­¢ None æŠ¥é”™
        for chunk in all_chunks:
            if "is_image" in chunk: del chunk["is_image"]
            if chunk.get("image_path") is None: chunk["image_path"] = ""
                
        vector_store.add_documents(all_chunks)
        print("âœ… å¤„ç†å®Œæˆï¼")
    else:
        print("âš ï¸ æœ¬æ¬¡æ²¡æœ‰ç”Ÿæˆä»»ä½•æ•°æ®ç‰‡æ®µã€‚")

if __name__ == "__main__":
    main()